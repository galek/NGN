-------------- Fragen
* Wie kann man sich nicht selbst in template spezialisierungen ertränken?
---------------------

Optionen in DOOM:
Lights Quality, Shadow Quality, Player Self-Shadow, Directional Occlusion Quality (AO), Decal Quality, Decal Filtering (Bi, Tri, Ani), Virtual Texturing Page Size, Reflections Quality, Particles Quality, Compute Shaders, Motion Blur Quality, Depth of Field, Depth of Field Antialiasing, HDR Bloom, Lens Flare, Lens Dirt

------------------------------
https://github.com/larspensjo/SimpleSignal Use this?

Sicherstellen, dass man das Mesh einfach nur als Mesh benutzen kann, ähnlich mit Shadern. Für irgendwelche Experimente möchte ich in der Lage sein, einfach nur ein Mesh + Shader zu rendern. SceneGraph + Renderer sollen optional sein. Vielleicht will man auch überlegen Materials ohne SceneGraph und Renderer zu nutzen? Eher nicht, weil ein Material einfach die Renderer-Agnostische-Magiclastige Version eines ShaderPrograms wird. Neben dem ShaderProgram muss man einfach eine Uniform-List oder so einbauen, die Uniform buffer bindet und dann ist man, denke ich, set.

Bei Object darauf achten (vllt. SceneNode nennen?), dass man render überschreiben kann und dann da eigene shenanigans machen kann. Auch drauf achten, dass man vielleicht trotzdem Materials benutzen kann. Vielleicht braucht man nen Material-Override?

Beschissenerweise muss man für den Depth-Prepass den Vertex-Shader mit einem anderen Fragment Shader (nur für depth) koppeln.
Außerdem will man nur bestimmte Vertex-Attribute binden
https://www.opengl.org/discussion_boards/showthread.php/183862-Depth-pre-pass-does-OpenGL-still-execute-fragment-shader-if-color-mask-is-GL_FALSE

Light-Attenuation 1D-Texture! Auch einfach für schnelleren Lookup und krasse Artist-Control

ein geerbtes object für LOD, dass mehrere meshes und materials nimmt, oder gleich mehrere objects (damit man shadows und so pro lod definieren kann), dann aber transformationen ignorieren

material-klasse - hält properties + maps, blending, depthTest, alphaTest, visible, backfaceculling
ShaderMaterial, MeshLambertMaterial

light-klasse (Object-derived) - color
PointLight, DirectionalLight, SpotLight, AmbientLight, HemisphereLight
light.shadow = new PCFShadowMap(), VarianceShadowMap()
darin kann man dann mapsizes und camera und sowas alles festlegen, die instanzieren zuerst ihre eigene camera, die dann entweder PerspectiveCamera oder OrthographicCamera ist, oder DualParaboloidCamera oder sowas

state-changes in Texture und Shader-class optimieren 
bei textures separat tracken in welche unit gebindet wurde/wird
Mehr weirde types unterstüzen in Textures

transforms
die camera-klasse soll man eigentlich nicht rendern können, ebenso soll sie auch kein mesh haben und keine children
object aufsplitten in Transforms (->camera), HasId (->camera), Renderable (bekommt children und render-methode und hash), Tagged (->camera)
http://en.cppreference.com/w/cpp/language/cast_operator - cast zu glm::mat4

Tagged hat eine findChild-Methode und addTag/removeTag methode, findChild ist optional rekursiv
findChild([](Tags tags){return tags.contains("dude") || tags.contains("peter");}); - sollte ne liste zurückgeben
findChild(const char*) - gibt eigentlich nur das erste zurück?

Mehr überlegen! Vielleicht will man trotzdem ids? Vielleicht will man trotzdem zusätzlich names?

Vielleicht trotzdem Ids

------

http://gamedev.stackexchange.com/questions/46424/try-catch-or-ifs-for-error-handling-in-c error handling

------

http://stackoverflow.com/questions/573025/which-3d-model-format-should-i-be-using
only support collada for importing (support it well), but write proprietary files - ngns (ngn scene)

Rather: Implement own format later (from/to COLLADA or Blender) and use ASSIMP for now

--------------------------------------
vertex attribute locations
http://stackoverflow.com/questions/4635913/explicit-vs-automatic-attribute-location-binding-for-opengl-shaders

http://gamedev.stackexchange.com/questions/57957/game-engine-design-ubershader-shader-management-design

Mittlerweile mag ich richtig, dass ich für jeden neuen Shader einen neuen VAO kompiliere. Einfach weil man dann bei missing attributes nichts extra durch die Gegend schickt (denke ich). Das sollte man aber Benchmarken!

## glGetUniformLocation und automatic attribute location assignment
Laut https://www.opengl.org/wiki/Vertex_Shader können die locations bei jedem linken anders sein.
Deshalb muss man für jede Mesh/Shader-Kombination einen eigenen VAO haben, eigentlich recht  lightweight, also nicht so schlimm, aber bei zwei versch. objekten muss man den schon jeden Frame neu erzeugen, in dem Fall ist das dann eher kacke.
Statt einer map[programID] ist es wohl besser einen vector zu haben, weil man ja immer so super wenige elemente hat. i.d.R halt 0 und höchstens 5 oder so.

------------------------------------
Für Instancing und und mehr Informationen vielleicht überlegen eine Zwischenabstraktion für ein Mesh-Material-Paar zu finden. z.B. Object und ObjectInstance (dieses objekt hält dann transformationen und so). 
Oder ObjectTemplate dazwischen einfügen. Macht automatisches Instancing und bucketing einfacher.

Vielleicht nennt man das Mesh/Material-Paar "Model"? Vielleicht macht man es mandatory?

Dann ist ein Mesh wirklich nur ein VAO und ein Model eine Referenz auf ein Mesh und ein Material.
Ein Object oder SceneNode hat dann ein Model und Instanzen von allen Uniforms, die dazu gehören (das heißt alle Variablen fürs Material)

Klarer wäre der Begriff "Model", wenn ein Model eine Hierarchie aus Mesh-Material paaren wäre (z.B. ein dude, der versch. Materials für sein Visier, sein T-Shirt und seine Haut hat.). Dann ist aber alles direkt super kompliziert. Im Prinzip sollte ein Model vielleicht ein Premade-SceneNode + Children sein? So nötig ist der Begriff Model ja auch nicht.

später gedacht:
Model ist absolut unnötig und es gibt kaum Grund es einzuführen. Für so Tree-Prefabs kann man "einfach" SceneNode.Load und SceneNode.Save implementieren.

-----------------------------------------------
### Other Engines
irrlicht.sourceforge.net/?page_id=291
https://github.com/turbulenz/turbulenz_engine
https://github.com/gameplay3d/GamePlay
https://github.com/gameplay3d/GamePlay/wiki/Tutorial%3A-Spaceship
https://github.com/panda3d/panda3d
https://www.panda3d.org/documentation.php
http://polycode.org/learn/scenes
http://www.horde3d.org/
http://www.horde3d.org/docs/html/_tutorial.html
https://libcinder.org/docs/
http://www.ogre3d.org/tikiwiki/tiki-index.php
https://github.com/urho3d/Urho3D/wiki/First%20Project
https://www.panda3d.org/documentation.php

https://bkaradzic.github.io/bgfx/examples.html
https://github.com/bkaradzic/bgfx/blob/master/examples/04-mesh/mesh.cpp
https://github.com/bkaradzic/bgfx/blob/master/examples/02-metaballs/metaballs.cpp
https://github.com/bkaradzic/bgfx/blob/master/examples/01-cubes/cubes.cpp
-----------------------------------------------

In Depth Pre-Pass strictly sort by depth, then in the full pass, don't sort by depth at all!

Still do light culling per object and globally (if the camera can see it and its not occluded) even in Forward Rendering and make it configurable how many lights per shader execution should be handled (still an array of lights probably?). And then do some passes.

http://aras-p.info/blog/2012/03/02/2012-theory-for-forward-rendering/
http://www.gamedev.net/topic/657701-most-efficient-way-for-lighting-in-forward-rendering/


https://www.opengl.org/discussion_boards/showthread.php/167939-Conditional-rendering

Unity macht Vertex-Lighting vielleicht auf CPU? Weit entfernte Lichter werden durch SH approximiert.
Nur Lichter, die nah und hell sind oder als "important" markiert werder per pixel (und dann je eins pro pass) berechnet.

## glBindAttribLocation

## numeric locations, set in GLSL and in the vertex format
Man muss einfach immer von Hand darauf aufpassen, dass die indizes stimmen

## layout-qualifier injection
Wenn ein Shader erstellt wird, werden alle Input-Attribute geparst und zu jedem Namen eine location assigned/gequeried und gespeichert, dann in den GLSL-Code injected oder mit glBindAttribLocation gebindet.
Wird ein VertexFormat erstellt, passiert das gleiche.

## hybrid
eine feste konvention für position, textur-koordinate, usw. haben, die vorgebaut ist (diesbezüglich wie die option davor), aber custom attribute fallen zurück auf glGetUniformLocation-Kram oder auf händisches assignment. beim händischen assignment gefällt mir nicht, dass die vertex-attribute keine namen mehr haben, sondern indizes vom type whatever (nicht richtig int)

vielleicht erstmal mit der ersten Methode fahren und dann später eine GLSL-Erweiterung schreiben, in der man 
!attribute type=vec3: position
!include: myfile.glsl
!include: myfile // works as well
!include: ngn.toLinearDepth // for helper functions
!uniform 
!glsl vertex:
void main() {
}
!glsl fragment:

#line benutzen vor allem was man einfügt, damit es stimmt

vielleicht versch. versionen für shader code zulassen, sodass man version-kompatiblen code schreiben kann (also für ogl2 oder 3 oder whatever)

ein material ist dann eine kombination aus so einem shader-programm und einem haufen uniforms?

parameter können einfach nur gesetzt sein, können aber auch eine value haben, die auch von anführungszeichen umgeben sein darf. inhalt des directives geht immer bis zum eof oder bis zum nächsten directive, das letzte newline wird ausgenommen

muss auch keine angst darum haben uniforms in beide shader zu kacken (werden sowieso wegoptimiert):
https://www.opengl.org/wiki/GLSL_:_common_mistakes

http://gamedev.stackexchange.com/questions/58515/engine-rendering-pipeline-making-shaders-generic

diese sprache kann auch für fullscreeneffekte verwendet werden
!pass: highpass
!glsl-fragment

!pass: 
!uniform type=pass: highpass

Every file should have to start with the first instruction !material or !postprocess

ngnml - ngn markup language
nesting ist obviously nicht möglich

das geparste objekt ist std::vector<Section>
Section = {std::string name, std::vector<std::pair<std::string, std::string> > attributes, std::string content}, content can be multi-line
easy to, I guess.

daraus kann man dann einen graphen bauen und alles mit render-targets regeln und so

siehe auch hier: https://github.com/spite/Wagner

Alternativ YAML - https://learnxinyminutes.com/docs/yaml/:
https://github.com/TwoLivesLeft/Codea-Documentation/blob/master/Shaders.yaml
https://docs.unity3d.com/Manual/YAMLSceneExample.html
https://mapzen.com/documentation/tangram/yaml/ !
https://github.com/jbeder/yaml-cpp
postprocess:
    passes:
        highpass:
            parameters:
                # This is taken from a central config registry
                threshold: !!config-variable bloomHighpassThresh
                input: !!input # The input of the whole postprocess
            shader:
                uniforms:
                    threshold: float
                    input: sampler2D
                glsl: |
                    if(length(input) > threshold) {
                        return input;
                    } else {
                        discard;
                    }
        blurh:
            # Defines for the shader
            defines:
                HORIZONTAL: true
            # Uniform values
            parameters:
                strength: 5
                input: !!pass-output highpass
            shader: !!include-file blur.yml 
        blurv:
            defines:
                HORIZONTAL: false
            parameters:
                strength: 5
                input: !!pass-output blurh
            shader: !!include-file blur.yml 
            output: !!screen

Maybe a shader block has to be thought through better
"Calling" a shader should have a fixed syntax (probably including defines, parameters = uniforms, etc.)

------------------------------------------------
Animation Blending und Morph (zwei Animationen auf versch. Teilen) - 
http://threejs.org/examples/webgl_animation_skinning_blending.html
http://threejs.org/examples/webgl_animation_skinning_morph.html

------------------------------------------------
VertexFormat Data-Driven angeben. In irgendwelchen Dateien (NGNML), die man in Objekt-Dateien (auch NGNML) referenzieren kann.

------------------------------------------------
Vermutlich möchte man Deferred Renderer und Forward Renderer zur gleichen Zeit benutzen. Letzteren für komplexe Materialien oder transparente Objekte. Die Frage ist: Wie kann man das supporten? Man kann einfach beide Renderer hintereinander aufrufen, aber man muss bestimmte Objekte abhängig vom Material auf bestimmte Renderer masken.

------------------------------------------------
Einfache, rudimentäre Collision Detection in 3D kann man super oft einfach mit raycasts machen!

-------------- scene file format
4 Bytes: NGNS # Magic
1 ui8: Major Version
1 ui8: Minor Version
[
    8 ui64: Chunk ID
    4 ui32: Total size // Total size and payload size so unknown chunks can be skipped
    4 ui32: Payload size
    ... : Payload
    [
        Child
    ]
    [
        Child
    ]
]

class MeshChunk {

};

class LightChunk {

};

class KeyValueChunk {
    std::map<std::string, std::string> values;
    KeyValueChunk(const NGNSChunk& chunk);
};

class NGNSChunk {
    uint64_t getId() const;
    uint32_t getPayloadSize() const;
    void* getPayloadData();
    std::vector<NGNSChunk> getChildren();
};

class NGNSFile {
    NGNSFile(const void* buffer);
    NGNSFile(const char* filename);
    std::pair<int, int> getVersion() const;
    NGNSChunk getRootChunk() {}

};

-- editor: schlomo


----------------------------------------- Old notes
Early-Z nur für Pixel die mit high-quality-shading gerendert werden (manchmal sind das nur die < ~50m von der kamera entfernten pixel)
occlusion queries kosten performance, wenn sowieso voll viel gerendert wird (z.b. wenn man sich ne stadt von oben anguckt), dann könnte man sie vielleicht disablen und auf was anderes fallbacken

man will OQs (occlusion queries) mit einfacherer geometrie rendern (vllt. bounding volumes), dann etwas unconditional rendern (terrain oder so, viewmodel, irgendwie sowas, was nicht geculled wird, damit die graka zeit hat) und dann conditional render

manche objekte will man als occluder rendern (ohne bounding volume), z.b. level geometrie, weil die konvexität das interessante ist, die will man weiterhin mit depth write rendern
occludee-test-geometrie (bounding boxes) will man mit depth-write off rendern

man will für depth-pre-passes front-to-back rendern, aber das sorting muss nicht 100% sein, sondern nur schnell

frustum cull your scene
sort visible objects coarsely front to back
render z-prepass (no fragment shading, color masking turned on), maybe just large occluders - terrain, large buildings and stuff
enable depth masking (disable z write and color write)
render scene again with only bounding volumes
write color and depth
render unconditional stuff
determine fragments that passed and render non-occluded objects

in release mode kein glGetError in jedem frame (könnte super langsamm implementiert sein), ansonsten in debug einfach in jedem frame

Gold links:
https://www.opengl.org/wiki/OpenGL_Loading_Library
https://www.opengl.org/wiki/History_of_OpenGL
http://stackoverflow.com/questions/4635913/explicit-vs-automatic-attribute-location-binding-for-opengl-shaders
https://www.opengl.org/wiki/Vertex_Specification_Best_Practices
https://www.opengl.org/wiki/Common_Mistakes#The_Object_Oriented_Language_Problem
http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-17-quaternions/
http://www.c-jump.com/bcc/common/Talk3/Math/GLM/GLM.html
http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-17-quaternions/

Vertex-Shader outputtet in clip space, dann wird durch clip.w dividiert -> ndc (normalized device coordinates) -> window coordinates durch transformation, wie in den docs von glViewport aufgeführt.


high-level-shit
für vbo-streaming double buffering
software-rendering für occlusion culling
custom allocators überall

RenderDoc!

https://www.reddit.com/r/gamedev/comments/4xhrce/what_is_grainy_lod_loading_and_why_is_it_used/
lod

Vielleicht öfter mal Deferred und Forward Rendering mischen. Hair or Skin in Cryengine forward rendered, rest deferred


Jedes GL command muss eigentlich in einem bestimmtem GL context passieren. Ich bin nicht sicher, ob man sich den Stress wirklich machen will. Vielleicht window zum singleton machen oder so?